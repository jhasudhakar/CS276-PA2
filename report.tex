\documentclass{article}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{parskip}

\geometry{letterpaper}

\begin{document}

\title{CS276 PA2 Report}

\author{
  Jiawei Yao\\
  \texttt{jwyao@stanford.edu}
  \and
  Wei Wei\\
  \texttt{wwei2@stanford.edu}
}

\maketitle

\section{System Design}

When designing the system, we put an emphasis on the following aspects:

\begin{itemize}
    \item \textbf{Modularity}: conform to good OOP practices; favor abstraction over implementation; reduce coupling among different components.
    \item \textbf{Flexibility}: it should be easy to add/remove implementation of a module without breaking other parts of the system.
    \item \textbf{Efficiency}: the system should be as performant as possible.
\end{itemize}

An example of modularity is we want to use \texttt{LanguageModel} as source of vocabulary/lexicon for \texttt{CandidateGenerator}, but we don't want to couple \texttt{CandidateGenerator} with the concrete \texttt{LanguageModel}. So we introduced an interface \texttt{Vocabulary} and let \texttt{LanguageModel} implement that interface.

Examples of flexibility include that we want to implement various smoothing techniques on language model, so we make \texttt{LanguageModel} an abstract base class and let subclasses to implement \texttt{bigramProbability}. With this change, we are able to add different smoothing techniques with few code.

The system is modularized into three parts:

\begin{enumerate}
    \item \textbf{Language Model}, used to compute the $P(Q)$ part of the noisy channel model.
    \item \textbf{Noisy Channel Model}, used to compute the $P(R|Q)$ part.
    \item \textbf{Candidate Generator}, used to generate possible candidates of $Q$ given $R$.
\end{enumerate}

Each part is documented in detail in the following sections, with design decisions addressing the above aspects.

\section{Language Model}

We referred to \cite{cs124-lm} and \cite{chen-goodman-99} for advanced smoothing techniques. There are 3 language models with different smoothing techniques in our implementation:

\begin{enumerate}
    \item \textbf{InterpolationLM}: implements the linear interpolation described in PA2 description.
    \item \textbf{AbsoluteDiscountLM}: implements absolute discounting described in \cite{chen-goodman-99}.
    \item \textbf{KneserNeyLM}: implements Kneser-Ney smoothing described in \cite{chen-goodman-99}.
\end{enumerate}

Specifically, since Kneser-Ney smoothing is an extension of absolute discounting, we implemented \texttt{KneserNeyLM} as a subclass of \texttt{AbsoluteDiscountLM}, which demonstrates clear modularization and extensibility of the system.

A specific optimization we found worthwhile when building the language model is to avoid compute all possible bigram probabilities. If all possible probabilities are computed, the memory limit will be exceeded, because the vocabulary size is huge. Instead, we store raw $count(w)$ and $count(w_{i-1}w_i)$ and compute the probability with the counts on the fly when needed. Storing counts also has the advantage that they can be used by advanced smoothing techniques to generate higher-order data, such as $n_1$ and $n_2$ used in absolute discounting and $N_{+}({\cdot}w_i)$ in Kneser-Ney smoothing.

On the other side, precomputing information of 1-dimension, higher-order data can greatly improve performance. For example, computing $N_{+}({\cdot}w_i)$ requires traversing all bigram counts. We can traverse the counts once and cache the result, or we can traverse the counts every time we need to compute $N_{+}({\cdot}w_i)$. Clearly, the former approach would be much faster as $|(w_{i-1},w_i): count(w_{i-1}w_i)>0|$ is large. This is an example of trading space for time.

\section{Noisy Channel Model}

Edit\cite{jm-book}\cite{kernighan-1990}

\section{Candidate Genenration}

Viterbi...

\section{Parameter Tuning}

Graphs...

\section{Extra Credit}

Yes, please!

\begin{thebibliography}{9}

\bibitem{chen-goodman-99}
    Chen, Stanley F and Goodman, Joshua.
    \emph{An empirical study of smoothing techniques for language modeling}.
    Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996.

\bibitem{kernighan-1990}
    Kernighan, Mark D., Kenneth W. Church, and William A. Gale.
    \emph{A spelling correction program based on a noisy channel model}.
    Proceedings of the 13th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, 1990.

\bibitem{jm-book}
    Jurafsky, Dan, and James H. Martin.
    \emph{Speech \& Language Processing}.
    Pearson Education India, 2000, pp. 163--168

\bibitem{cs124-lm}
    Jurafsky, Dan.
    \emph{Language Modeling}.
    2014, available at \url{http://www.stanford.edu/class/cs124/lec/languagemodeling.pdf}
\end{thebibliography}

\end{document}
