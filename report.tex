\documentclass{article}

\usepackage{enumerate}
\usepackage[bottom=1in,top=1in]{geometry}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[pdftex,colorlinks,urlcolor=blue]{hyperref}

\geometry{letterpaper}

\begin{document}

\title{CS276 PA2 Report\footnote{We apologize for submitting a report longer than 2 pages. We tried to reduce the length as we can, and we believe our design and experiment process cannot be fully explained without these pages. Thank you for your time.}}

\author{
  Jiawei Yao\\
  \texttt{jwyao@stanford.edu}
  \and
  Wei Wei\\
  \texttt{wwei2@stanford.edu}
}

\maketitle

\section{System Design}

When designing the system, we put an emphasis on the following aspects:

\begin{itemize}
    \item \textbf{Modularity}: conform to good OOP practices; favor abstraction over implementation; reduce coupling among different components.
    \item \textbf{Flexibility}: it should be easy to add/remove implementation of a module without breaking other parts of the system.
    \item \textbf{Efficiency}: the system should be as performant as possible.
\end{itemize}

An example of modularity is we want to use \texttt{LanguageModel} as source of vocabulary/lexicon for \texttt{CandidateGenerator}, but we don't want to couple \texttt{CandidateGenerator} with the concrete \texttt{LanguageModel}. So we introduced an interface \texttt{Vocabulary} and let \texttt{LanguageModel} implement that interface.

Examples of flexibility include that we want to implement various smoothing techniques on language model, so we make \texttt{LanguageModel} an abstract base class and let subclasses to implement \texttt{bigramProbability}. With this change, we are able to add different smoothing techniques with few code.

\section{Language Model}

We referred to \cite{cs124-lm} and \cite{chen-goodman-99} for advanced smoothing techniques. There are 3 language models with different smoothing techniques in our implementation:

\begin{enumerate}
    \item \textbf{InterpolationLM}: implements the linear interpolation described in PA2 description.
    \item \textbf{AbsoluteDiscountLM}: implements absolute discounting described in \cite{chen-goodman-99}.
    \item \textbf{KneserNeyLM}: implements Kneser-Ney smoothing described in \cite{chen-goodman-99}.
\end{enumerate}

Specifically, since Kneser-Ney smoothing is an extension of absolute discounting, we implemented \texttt{KneserNeyLM} as a subclass of \texttt{AbsoluteDiscountLM}, which demonstrates clear modularization and extensibility of the system.

A specific optimization we found worthwhile when building the language model is to avoid compute all possible bigram probabilities. If all possible probabilities are computed, the memory limit will be exceeded, because the vocabulary size is huge. Instead, we store raw $count(w)$ and $count(w_{i-1}w_i)$ and compute the probability with the counts on the fly when needed. Storing counts also has the advantage that they can be used by advanced smoothing techniques to generate higher-order data, such as $n_1$ and $n_2$ used in absolute discounting and $N_{+}({\cdot}w_i)$ in Kneser-Ney smoothing.

On the other side, precomputing information of 1-dimension, higher-order data can greatly improve performance. For example, computing $N_{+}({\cdot}w_i)$ requires traversing all bigram counts. We can traverse the counts once and cache the result, or we can traverse the counts every time we need to compute $N_{+}({\cdot}w_i)$. Clearly, the former approach would be much faster as $|(w_{i-1},w_i): count(w_{i-1}w_i)>0|$ is large. This is an example of trading space for time.

\section{Channel Model}

The uniform cost model is trivial and the choice of uniform cost value is discussed in the Parameter Tuning section.

The empirical cost model (ECM) is more interesting. We referred to \cite{jm-book} and \cite{kernighan-1990} to get a good understanding of what's going on here.

Like language model, the training process of ECM is actually just gathering the following counts: $del[w_{i-1},w_i]$, $ins[w_{i-1},x_i]$, $sub[x_i,w_i]$, $trans[w_{i-1},w_i]$, $count[w_i]$ and $count[w_{i-1},w_i]$. With these counts, we can easily compute different edit probability in $O(1)$ time\footnote{We use \texttt{HashMap} for fast lookup.}, according to the formulae from \cite{kernighan-1990}.

As we are following \cite{kernighan-1990} and conditioning edits on the previous character, we introduce a meta charater \texttt{\$} as the ``beginning of sentence" character. Another caveat is that there's no guarantee that all character in the test queries appear in the corpus of ECM's training data. To address this problem, we add another meta character into ECM's alphabet\footnote{this character is chosen to be different from any characters in candidate generator's alphabet}. All character not in ECM's alphabet will be mapped to the unknown character. And since unknown character might never appear in the training data, we apply Laplace smoothing when computing the conditional probabilities.

We appled two optimizations in the ECM. The first is using an $O(N)$ algorithm to determine edits in the training instances. A generic edit distance algorithm would be $O(MN)$, but since it's guaranteed that the each training instance has exactly one edit in it, we can use a linear complexity algorithm to determine the edit more efficiently.

The second optimization is that we use Damerau-Levenshtein edit distance algorithm\footnote{To be precise, we use dynamic programming to determine minimal number of edits and record what kind of edit causes the minimal change as the algorithm runs. An $O(max(M,N))$ algorithm is used to determine the actual edits. See: \url{http://en.wikipedia.org/wiki/Damerau-Levenshtein\_distance}} for determining edits from $Q$ to $R$. You may think this step can totally be avoided if we record the edits while generating candidates. It's true. But as only candidates in the dictionary will be selected, the possibility that a candidate be actually computed with the noisy channel model is very low. If we doing so, much of the edits recorded in the candidate generating phase will be useless, which hurts performance. Moreover, recording edits may not be feasible for every candidate generation algorithm. Suppose we'd use a transducer to generate candidates, recording edits while generating might complicate the already complicated algorithm. In a word, for better modularity and maximal flexibility, we defer the edits determination process in ECM.

\section{Candidate Genenration}

We made an assumption that each query is 2 edit distance away from the correct query and each word is at most 1 edit distance away. After making this assumption, we can generate candidates pretty efficiently. We first generate all edit distance 1 candidates (alphabet includes spaces so that we can split words in the query). Then we use a dictionary to filter the candidates we genereated. After that, we generate edit distance 1 candidates of remaining candidates. It is pretty efficient, we can finish our correction in 300 seconds.

\section{Parameter Tuning}

We tried to tune parameters to achieve the best performance. To avoid overfitting, before we conducted parameter tuning, we splitted the training data into a development set and a test set with a 7:3 ratio\footnote{See \texttt{split.py}}.

\begin{figure}[!htb]
  \centering
  \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{normal.pdf}
      \caption{Without extra part}
      \label{fig:mu-normal}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=\linewidth]{extra.pdf}
      \caption{With extra part}
      \label{fig:mu-extra}
  \end{subfigure}
  \caption{Tuning $\mu$}
  \label{fig:mu}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=8cm]{uniform_edit_cost.pdf}
  \caption{Tuning uniform edit cost}
  \label{fig:uniform-edit-cost}
\end{figure}

Figure.~(\ref{fig:mu-normal}) and Figure.~(\ref{fig:mu-extra}) shows our process of finding the best mu for our program. For basic credit, the best value of mu is 1.3 (empirical) 0.5 (uniform). For our extra credit program, empirical, 1.0 is the best value and for uniform model, 0.5 is the best value. (lambda is $0.05$ in our interpolation model, uniform edit cost is 0.03)

As for uniform edit cost, we found that 0.03 is suitable for our program. Figure.~(\ref{fig:uniform-edit-cost}) confirms that.

\section{Extra Credit}

We tried different smoothing techniques and implemented modified Viterbi algorithm for performance boost.

\subsection{Advanced Smoothing}

We implemented Absolute Discounting and Kneser-Ney Smoothing. When \texttt{extra} parameter is passed to the script, Kneser-Ney is used. For details of the smoothing, please refer to the Language Model section.

\subsection{Viterbi algorithm}

We implemented Viterbi algorithm\footnote{\url{http://en.wikipedia.org/wiki/Viterbi\_algorithm}} to compute scores of each candidate. It is extremely fast. If a query has N words, and each word has M candidates, then our algorithm can compute scores for different combinations in $O(NM^2)$ time. One thing to note is that we can choose to store small number of intermediate results for each word so that our algorithm's time complexity is now $O(NMk)$ where k is the number of candidates we store for each word.

Since Viterbi algorithm is extremely fast, now we can genereate a lot of candidates for each word! If we generate edit distance 1 away candidates, potentially, we can correct queries that have several edit distance 1 away errors. Now the computation bottleneck is in candidate generation. If we have more time, we'd like to try k-gram to generate candidates much more efficiently.

Some results after apply Viterbi algorithm: If we only consider edit distance 1 candidates, we can correct 454 queries in 29s and achieve an accuracy of 88.79\%. If we consider all edit distance 2 candidates, we can correct all queries in 666 seconds and achieve an accuracy of 91.86\%.

Please refer to \texttt{RunCorrect.java} for more details of our Viterbi algorithm.

\begin{thebibliography}{9}

\bibitem{chen-goodman-99}
    Chen, Stanley F and Goodman, Joshua.
    \emph{An empirical study of smoothing techniques for language modeling}.
    Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996.

\bibitem{kernighan-1990}
    Kernighan, Mark D., Kenneth W. Church, and William A. Gale.
    \emph{A spelling correction program based on a noisy channel model}.
    Proceedings of the 13th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, 1990.

\bibitem{jm-book}
    Jurafsky, Dan, and James H. Martin.
    \emph{Speech \& Language Processing}.
    Pearson Education India, 2000, pp. 163--168

\bibitem{cs124-lm}
    Jurafsky, Dan.
    \emph{Language Modeling}.
    2014, available at \url{http://www.stanford.edu/class/cs124/lec/languagemodeling.pdf}
\end{thebibliography}

\end{document}
