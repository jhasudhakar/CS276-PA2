\documentclass{article}

\usepackage{enumerate}
\usepackage{hyperref}
\usepackage[bottom=1in,top=1in]{geometry}
\usepackage{parskip}

\geometry{letterpaper}

\begin{document}

\title{CS276 PA2 Report\footnote{We apologize for submitting a report longer than 2 pages. We tried to reduce the length as we can, and we believe our design and experiment process cannot be fully explained without these pages. Thank you for your time.}}

\author{
  Jiawei Yao\\
  \texttt{jwyao@stanford.edu}
  \and
  Wei Wei\\
  \texttt{wwei2@stanford.edu}
}

\maketitle

\section{System Design}

When designing the system, we put an emphasis on the following aspects:

\begin{itemize}
    \item \textbf{Modularity}: conform to good OOP practices; favor abstraction over implementation; reduce coupling among different components.
    \item \textbf{Flexibility}: it should be easy to add/remove implementation of a module without breaking other parts of the system.
    \item \textbf{Efficiency}: the system should be as performant as possible.
\end{itemize}

An example of modularity is we want to use \texttt{LanguageModel} as source of vocabulary/lexicon for \texttt{CandidateGenerator}, but we don't want to couple \texttt{CandidateGenerator} with the concrete \texttt{LanguageModel}. So we introduced an interface \texttt{Vocabulary} and let \texttt{LanguageModel} implement that interface.

Examples of flexibility include that we want to implement various smoothing techniques on language model, so we make \texttt{LanguageModel} an abstract base class and let subclasses to implement \texttt{bigramProbability}. With this change, we are able to add different smoothing techniques with few code.

The system is modularized into three parts:

\begin{enumerate}
    \item \textbf{Language Model}, used to compute the $P(Q)$ part of the noisy channel model.
    \item \textbf{Channel Model}, used to compute the $P(R|Q)$ part.
    \item \textbf{Candidate Generator}, used to generate possible candidates of $Q$ given $R$.
\end{enumerate}

Each part is documented in detail in the following sections, with design decisions addressing the above aspects.

\section{Language Model}

We referred to \cite{cs124-lm} and \cite{chen-goodman-99} for advanced smoothing techniques. There are 3 language models with different smoothing techniques in our implementation:

\begin{enumerate}
    \item \textbf{InterpolationLM}: implements the linear interpolation described in PA2 description.
    \item \textbf{AbsoluteDiscountLM}: implements absolute discounting described in \cite{chen-goodman-99}.
    \item \textbf{KneserNeyLM}: implements Kneser-Ney smoothing described in \cite{chen-goodman-99}.
\end{enumerate}

Specifically, since Kneser-Ney smoothing is an extension of absolute discounting, we implemented \texttt{KneserNeyLM} as a subclass of \texttt{AbsoluteDiscountLM}, which demonstrates clear modularization and extensibility of the system.

A specific optimization we found worthwhile when building the language model is to avoid compute all possible bigram probabilities. If all possible probabilities are computed, the memory limit will be exceeded, because the vocabulary size is huge. Instead, we store raw $count(w)$ and $count(w_{i-1}w_i)$ and compute the probability with the counts on the fly when needed. Storing counts also has the advantage that they can be used by advanced smoothing techniques to generate higher-order data, such as $n_1$ and $n_2$ used in absolute discounting and $N_{+}({\cdot}w_i)$ in Kneser-Ney smoothing.

On the other side, precomputing information of 1-dimension, higher-order data can greatly improve performance. For example, computing $N_{+}({\cdot}w_i)$ requires traversing all bigram counts. We can traverse the counts once and cache the result, or we can traverse the counts every time we need to compute $N_{+}({\cdot}w_i)$. Clearly, the former approach would be much faster as $|(w_{i-1},w_i): count(w_{i-1}w_i)>0|$ is large. This is an example of trading space for time.

\section{Channel Model}

The uniform cost model is trivial and the choice of uniform cost value is discussed in the Parameter Tuning section.

The empirical cost model (ECM) is more interesting. We referred to \cite{jm-book} and \cite{kernighan-1990} to get a good understanding of what's going on here.

Like language model, the training process of ECM is actually just gathering the following counts: $del[w_{i-1},w_i]$, $ins[w_{i-1},x_i]$, $sub[x_i,w_i]$, $trans[w_{i-1},w_i]$, $count[w_i]$ and $count[w_{i-1},w_i]$. With these counts, we can easily compute different edit probability in $O(1)$ time\footnote{We use \texttt{HashMap} for fast lookup.}, according to the formulae from \cite{kernighan-1990}.

As we are following \cite{kernighan-1990} and conditioning edits on the previous character, we introduce a meta charater \texttt{\$} as the ``beginning of sentence" character. Another caveat is that there's no guarantee that all character in the test queries appear in the corpus of ECM's training data. To address this problem, we add another meta character into ECM's alphabet\footnote{this character is chosen to be different from any characters in candidate generator's alphabet}. All character not in ECM's alphabet will be mapped to the unknown character. And since unknown character might never appear in the training data, we apply Laplace smoothing when computing the conditional probabilities.

We appled two optimizations in the ECM. The first is using an $O(n)$ algorithm to determine edits in the training instances. A generic edit distance algorithm would be $O(mn)$, but since it's guaranteed that the each training instance has exactly one edit in it, we can use a linear complexity algorithm to determine the edit more efficiently.

The second optimization is that we use Damerau-Levenshtein edit distance algorithm\footnote{To be precise, we use dynamic programming to determine minimal number of edits and record what kind of edit causes the minimal change as the algorithm runs. An $O(max(m,n))$ algorithm is used to determine the actual edits. See: \url{http://en.wikipedia.org/wiki/Damerau-Levenshtein\_distance}} for determining edits from $Q$ to $R$. You may think this step can totally be avoided if we record the edits while generating candidates. It's true. But as only candidates in the dictionary will be selected, the possibility that a candidate be actually computed with the noisy channel model is very low. If we doing so, much of the edits recorded in the candidate generating phase will be useless, which hurts performance. Moreover, recording edits may not be feasible for every candidate generation algorithm. Suppose we'd use a transducer to generate candidates, recording edits while generating might complicate the already complicated algorithm. In a word, for better modularity and maximal flexibility, we defer the edits determination process in ECM.

\section{Candidate Genenration}

Viterbi...

\section{Parameter Tuning}

We tried to tune parameters to achieve the best performance. To avoid overfitting, before we conducted parameter tuning, we splitted the training data into a development set and a test set with a 7:3 ratio\footnote{See \texttt{split.py}}.

Graphs...

\section{Extra Credit}

Yes, please!

\begin{thebibliography}{9}

\bibitem{chen-goodman-99}
    Chen, Stanley F and Goodman, Joshua.
    \emph{An empirical study of smoothing techniques for language modeling}.
    Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996.

\bibitem{kernighan-1990}
    Kernighan, Mark D., Kenneth W. Church, and William A. Gale.
    \emph{A spelling correction program based on a noisy channel model}.
    Proceedings of the 13th conference on Computational linguistics-Volume 2. Association for Computational Linguistics, 1990.

\bibitem{jm-book}
    Jurafsky, Dan, and James H. Martin.
    \emph{Speech \& Language Processing}.
    Pearson Education India, 2000, pp. 163--168

\bibitem{cs124-lm}
    Jurafsky, Dan.
    \emph{Language Modeling}.
    2014, available at \url{http://www.stanford.edu/class/cs124/lec/languagemodeling.pdf}
\end{thebibliography}

\end{document}
